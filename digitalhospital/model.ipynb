{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdaf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Dosya yolu (senin belirttiÄŸin gibi)\n",
    "input_path = \"C:/Users/EKIN/Desktop/Combined Data.csv\"\n",
    "\n",
    "# CSV dosyasÄ±nÄ± yÃ¼kle\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Gereksiz 'Unnamed: 0' sÃ¼tunu varsa sil\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# 'statement' ve 'status' sÃ¼tunlarÄ±nda boÅŸ deÄŸerleri temizle\n",
    "df.dropna(subset=['statement', 'status'], inplace=True)\n",
    "\n",
    "print(f\" Veri yÃ¼klendi. Kalan satÄ±r sayÄ±sÄ±: {df.shape[0]}\")\n",
    "\n",
    "# Metin temizleme fonksiyonu\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # linkleri sil\n",
    "    text = re.sub(r'<.*?>+', '', text)  # html etiketleri sil\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # noktalama sil\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # sayÄ± iÃ§eren kelimeleri sil\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # ekstra boÅŸluklarÄ± sil\n",
    "    return text\n",
    "\n",
    "# TemizlenmiÅŸ metinleri yeni sÃ¼tuna ekle\n",
    "df['clean_text'] = df['statement'].astype(str).apply(clean_text)\n",
    "\n",
    "# Ã–rnek ilk 5 temiz metni gÃ¶ster\n",
    "print(\"\\nTemizlenmiÅŸ metin Ã¶rnekleri:\")\n",
    "print(df['clean_text'].head())\n",
    "df.to_csv(\"C:/Users/EKIN/Desktop/temizlenmis_veri.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# Veri yÃ¼kleme\n",
    "data_path = \"C:/Users/EKIN/Desktop/temizlenmis_veri.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\" Veri dosyasÄ± bulunamadÄ±: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Gerekli sÃ¼tunlar var mÄ± kontrol et\n",
    "required_columns = [\"clean_text\", \"status\"]\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"'{col}' sÃ¼tunu eksik! LÃ¼tfen veriyi kontrol et.\")\n",
    "\n",
    "#  MPNet modelini yÃ¼kle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "\n",
    "# Temiz metinleri al\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "# Embed iÅŸlemi batch'lerle yapÄ±lacak\n",
    "batch_size = 64\n",
    "chunks = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "X_embed_chunks = []\n",
    "\n",
    "print(f\"\\n Embed iÅŸlemi {len(chunks)} parÃ§ada yapÄ±lacak...\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    start = time.time()\n",
    "    embeds = model.encode(chunk, batch_size=batch_size, show_progress_bar=False)\n",
    "    X_embed_chunks.append(embeds)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\" Chunk {i}/{len(chunks)} iÅŸlendi. ({elapsed:.2f} saniye)\")\n",
    "\n",
    "# ðŸ”— VektÃ¶rleri birleÅŸtir\n",
    "X_embed = np.vstack(X_embed_chunks)\n",
    "\n",
    "#  Etiketleri sayÄ±ya Ã§evir\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df[\"status\"].values)\n",
    "\n",
    "#  Kaydetme yollarÄ±\n",
    "embedding_npy_path = \"C:/Users/EKIN/Desktop/mpnet_embeddings.npy\"\n",
    "label_npy_path = \"C:/Users/EKIN/Desktop/mpnet_labels.npy\"\n",
    "embedding_csv_path = \"C:/Users/EKIN/Desktop/mpnet_embeddings.csv\"\n",
    "encoder_save_path = \"C:/Users/EKIN/Desktop/label_encoder.pkl\"\n",
    "\n",
    "# KayÄ±t iÅŸlemleri\n",
    "np.save(embedding_npy_path, X_embed)\n",
    "np.save(label_npy_path, y_encoded)\n",
    "joblib.dump(le, encoder_save_path)\n",
    "\n",
    "#  CSV olarak da kayÄ±t\n",
    "X_embed_df = pd.DataFrame(X_embed)\n",
    "X_embed_df[\"label\"] = y_encoded\n",
    "X_embed_df.to_csv(embedding_csv_path, index=False)\n",
    "\n",
    "print(\"\\n Embed iÅŸlemi tamamlandÄ±.\")\n",
    "print(f\" Kaydedilen dosyalar:\\n- {embedding_npy_path}\\n- {label_npy_path}\\n- {embedding_csv_path}\\n- {encoder_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53416283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Embed dosyasÄ±nÄ± yÃ¼kle\n",
    "embedding_npy_path = \"C:/Users/EKIN/Desktop/mpnet_embeddings.npy\"\n",
    "X_embed = np.load(embedding_npy_path)\n",
    "\n",
    "# PCA nesnesi oluÅŸtur (Ã¶rnek: 400 boyut)\n",
    "pca = PCA(n_components=400, random_state=42)\n",
    "\n",
    "# PCA uygulama\n",
    "X_reduced = pca.fit_transform(X_embed)\n",
    "\n",
    "# Kaydetme yollarÄ±\n",
    "pca_npy_path = \"C:/Users/EKIN/Desktop/mpnet_embeddings_pca.npy\"\n",
    "pca_model_path = \"C:/Users/EKIN/Desktop/pca_model.pkl\"\n",
    "\n",
    "# PCA sonucu kaydet\n",
    "np.save(pca_npy_path, X_reduced)\n",
    "joblib.dump(pca, pca_model_path)\n",
    "\n",
    "print(\" PCA ile boyut indirgeme tamamlandÄ±.\")\n",
    "print(f\" Kaydedilen dosyalar:\\n- {pca_npy_path}\\n- {pca_model_path}\")\n",
    "print(f\" Yeni boyut: {X_reduced.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Etiketleri yÃ¼kle\n",
    "label_npy_path = \"C:/Users/EKIN/Desktop/mpnet_labels.npy\"\n",
    "y = np.load(label_npy_path)\n",
    "\n",
    "# SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± yazdÄ±r\n",
    "print(\"Orijinal sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\", Counter(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# PCA uygulanmÄ±ÅŸ veriyi yÃ¼kle\n",
    "X = np.load(\"C:/Users/EKIN/Desktop/mpnet_embeddings_pca.npy\")\n",
    "y = np.load(\"C:/Users/EKIN/Desktop/mpnet_labels.npy\")\n",
    "\n",
    "# SMOTE uygulanÄ±yor\n",
    "print(\" SMOTE uygulanÄ±yor...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(f\"SMOTE sonrasÄ± yeni veri boyutu: {X_resampled.shape}\")\n",
    "print(f\" Yeni sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±: {Counter(y_resampled)}\")\n",
    "\n",
    "# Kaydet\n",
    "np.save(\"C:/Users/EKIN/Desktop/X_resampled.npy\", X_resampled)\n",
    "np.save(\"C:/Users/EKIN/Desktop/y_resampled.npy\", y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Verileri yÃ¼kle\n",
    "X = np.load(\"C:/Users/EKIN/Desktop/X_resampled.npy\")\n",
    "y = np.load(\"C:/Users/EKIN/Desktop/y_resampled.npy\")\n",
    "\n",
    "#  EÄŸitim ve test seti\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#  TensÃ¶rlere Ã§evir\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "#  Veri yÃ¼kleyiciler\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=128)\n",
    "\n",
    "#  Model tanÄ±mÄ±\n",
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.out = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "num_classes = len(np.unique(y))\n",
    "model = DeepModel(input_dim, num_classes)\n",
    "\n",
    "#  EÄŸitim ayarlarÄ±\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#  EÄŸitimi baÅŸlat\n",
    "for epoch in range(10):  # epoch sayÄ±sÄ±nÄ± ayarlayabilirsin\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1} tamamlandÄ± âœ…\")\n",
    "\n",
    "#  Test performansÄ±\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    true = []\n",
    "    for xb, yb in test_loader:\n",
    "        output = model(xb)\n",
    "        pred = torch.argmax(output, axis=1)\n",
    "        preds.extend(pred.numpy())\n",
    "        true.extend(yb.numpy())\n",
    "\n",
    "acc = accuracy_score(true, preds)\n",
    "f1 = f1_score(true, preds, average=\"weighted\")\n",
    "print(f\" PyTorch Model Accuracy: {acc:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "#  Modeli kaydet\n",
    "torch.save(model.state_dict(), \"C:/Users/EKIN/Desktop/deep_model.pt\")\n",
    "print(\" Model baÅŸarÄ±yla kaydedildi: deep_model.pt\")\n",
    "\n",
    "#  Tahmin iÃ§in fonksiyon\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def predict_from_text_pytorch(text, encoder_path, pca_path, model_path, embed_model):\n",
    "    text_clean = clean_text(text)\n",
    "    vector = embed_model.encode([text_clean])\n",
    "    pca = joblib.load(pca_path)\n",
    "    vector_pca = pca.transform(vector)\n",
    "\n",
    "    label_encoder = joblib.load(encoder_path)\n",
    "    model = DeepModel(input_dim=vector_pca.shape[1], num_classes=len(label_encoder.classes_))\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(vector_pca, dtype=torch.float32)\n",
    "        output = model(x)\n",
    "        predicted = torch.argmax(output, axis=1).item()\n",
    "\n",
    "    return label_encoder.inverse_transform([predicted])[0]\n",
    "\n",
    "#  Ã–rnek cÃ¼mle tahmini\n",
    "mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "test_text = \"Iâ€™m feeling so anxious and overwhelmed lately.\"\n",
    "predicted_label = predict_from_text_pytorch(\n",
    "    test_text,\n",
    "    encoder_path=\"C:/Users/EKIN/Desktop/label_encoder.pkl\",\n",
    "    pca_path=\"C:/Users/EKIN/Desktop/pca_model.pkl\",\n",
    "    model_path=\"C:/Users/EKIN/Desktop/deep_model.pt\",\n",
    "    embed_model=mpnet\n",
    ")\n",
    "print(f\"\\nðŸ’¡ PyTorch Modeli Tahmini: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589a100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655f050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
